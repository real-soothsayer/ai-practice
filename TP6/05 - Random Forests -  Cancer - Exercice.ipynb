{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Random Forests </center>\n",
    "We are going to use ensembles which combines multiple machine learning modules into a more powerful model.\n",
    "The first model to explore is Random Forests.\n",
    "We can think of a Random Forest as a collection of decision trees.\n",
    "<br/><br/>\n",
    "**How Random Forests are better or different from single decision trees ?**\n",
    "We've seen the decision trees tend to overfit on the part of the data. combining multiple trees retains their predictive power and it can reduce overfitting by averaging the results.<br/>\n",
    "A distinguishing powerful feature of Random Forests is that it applies randomness when building each tree. The major parameter to specify is estimators which refers to how many trees to create.\n",
    "\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and test the model :\n",
    "1. Split the data.\n",
    "2. Instanciate the Random Forest Classifier with the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Fill here\n",
    "\n",
    "# X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's evaluate the algorithm :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "97% of accuracy on the test data with the default settings is pretty good !\n",
    "We could obtain possibly better results by adjusting other parameters like :\n",
    "* Max features which controls the randomness of each tree.\n",
    "* Pre-pruning which is similar to what is done in single trees.\n",
    "\n",
    "Feature importance is more representative than in single trees, it provides a more balnced overview of feature weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "n_features = cancer.data.shape[1]\n",
    "\n",
    "n_features = cancer.data.shape[1]\n",
    "plt.barh(range(n_features), forest.feature_importances_, align='center')\n",
    "plt.yticks(np.arange(n_features), cancer.feature_names)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In single trees we've seen that worst radius feature carried a significantly highr weight compared to the rest of the features which were 30.\n",
    "With Random Forests we can see that many more features have a non-zero contribution, they play a heavier role in decision making compared to the single decision tree. Random Forsts, in our case, appears to provide a more informed choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Advantages of Random Forests\n",
    "\n",
    " - Powerful and widely implemented.\n",
    " - Perform well with default settings.\n",
    " - Don't require scaling of the data.\n",
    " - Randomization makes them better than single DT.\n",
    " \n",
    "### Parameters to Tune\n",
    " \n",
    " - n_jobs : Number of cores to use for training (n_jobs=-1, for all cores), using two cores will double the speed compared to only using one core,\n",
    " - n_estimators : How many trees to use. More is always better, it reduces overfitting, BUT consider the training time and memory allocation, more trees drain more computer resources, \n",
    " - max_depth, for pre-pruning,\n",
    " - max_features, for randomization. Default values are :\n",
    "     - max_features = sqrt(n_features), for classification\n",
    "     - max_features = log2(n_features), for regression\n",
    " - etc.\n",
    " \n",
    "### Potential Disadvantages of Random Forests\n",
    "\n",
    "- Not so good performance on very high dimensional and sparse data (text data).\n",
    "- Large datasets require more resources for training (time, CPUs, etc).\n",
    "- Cannot be visualized as well as single Decision Tree."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
